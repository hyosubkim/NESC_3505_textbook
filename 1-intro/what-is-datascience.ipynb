{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45e5f50a-e85e-4ca2-8498-89a0835485b2",
   "metadata": {},
   "source": [
    "# What is Data Science?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c304ba-1e8a-4d62-ad68-d3948bf5239b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This chapter is largely copied and adapted from Aaron Newman's <a href=\"https://neuraldatascience.io/intro.html\">Neural Data Science textbook</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f52be7-beb8-4a2b-92b7-c8baaaf67add",
   "metadata": {},
   "source": [
    "Many areas of modern scientific research and development rely on increasingly large and complex data sets. Discovery and application in science thus relies on the ability to manage these large data sets, and extract meaning from them. In other words, neuroscience now relies heavily on **data science**, which has been variously defined as “…an umbrella term to describe the entire complex and multistep processes used to extract value from data” (Wing, 2019), and the ability to “bring structure to large quantities of formless data and make analysis possible” (Davenport & Patil, 2012, p.73).\n",
    "\n",
    "**In modern kinesiological research, data science is an increasingly necessary and valued skill.** Data from techniques like EMG, motion capture, and MRI are complex and multidimensional. Being able to understand, manipulate, and visualize the structure of these complex datasets is a necessary skill for performing the research. On top of this, it is increasingly clear that very large data sets - often built collaboratively by many labs - are often required to make reliable inferences about scientific processes.\n",
    "\n",
    "## Is data science just a trendy name for statistics?\n",
    "While data science and statistics are overlapping fields, statistics is generally focused on the specific task of testing hypotheses based on data. Data science more broadly includes the storage, manipulation, visualization, filtering, and preparation of data that is typically required prior to statistical analysis. In fact, we will, by necessity, be devoting more time in this course to the pre-data analysis steps than to actual algorithms (although there will also be some of that as well). Data science does also encompass statistics, as well as machine learning; whereas statistics generally involves deriving conclusions from existing data, machine learning involves making predictions from a data set that will generalize to other data. Since statistics is covered in other courses in the kinesiology curricula, this course focuses instead on the other “front-end” aspects of data science described above. Other areas of data science, including software development and “back-end” data science (engineering, hardware, databases), will not be covered.\n",
    "\n",
    "This highlights a mindset that differs quite dramatically in data science, as compared to the basic statistics taught in undergraduate curricula. Data science includes practices that are more exploratory. In experimentally-oriented disciplines such as exercise physiology or biomechanics, statistics are a natural approach to deriving meaning from data. This is because data typically come from experiments, in which the researcher(s) systematically and intentionally manipulated certain variables. A good experiment is **hypothesis-driven**, meaning that the researcher has predictions in advance as to how the data will systematically vary with the experimental manipulations. These predictions are usually based on past experimental findings, or models of the process being studied. Statistics are fundamentally embedded in data science — and indeed, the concept of \"data science\" as a discipline emerged from the field of statistics — but data science can be thought of as a larger set of practices the includes statistics, machine learning, data cleaning and transformations, and visualization. Many of these approaches are more **exploratory** than hypothesis-driven. That is, rather than looking for a specific, predicted pattern, the data scientist explores the data to find systematic patterns that may emerge from the data. For example, researchers using techniques like motion capture have attempted to use machine learning algorithms to detect subtle alterations in walking mechanics, as a means of one day being able to \"classify\" clinically asymptomatic people as being at higher risk of developing neurologic disease, such as Parkinson's. \n",
    "\n",
    "## Tools for Data Science\n",
    "Central to data science is the ability to use scientific programming languages, such as Python, Matlab, and R. This ability includes a strong understanding of the fundamentals of at least one programming language, and the ability to extend one’s knowledge through continuous learning and problem-solving. This course teaches Python, a mature and widely-used language in modern scientific research and data science more broadly. However, many of the fundamentals of scientific programming and data science are common to many languages. Thus, having learned Python, you will be well-prepared to learn new languages in the future, as necessary.\n",
    "\n",
    "Another important facet of data science is that it is a **team endeavour**. On the one hand, it is founded on open, shared software developed by widely distributed teams of contributors. On the other hand, the practice of data science typically involves teams of individuals with complementary skillsets, both due to the size and complexity of many projects. In science, these teams often comprise students and faculty members in collaborating labs distributed around the world. Team members with different skillsets can also teach each other new things, often through demonstration in a shared project. This class prepares you for such collaboration by developing and coaching your teamwork skills, as well as teaching you how to use software platforms that support such collaboration.\n",
    "\n",
    "The skills learned in this class will benefit students working in a wide range of areas of kinesiology. As well, the class will provide an introductory foundation in data science that can be applied to a range of areas beyond kinesiology, in academia, industry, and government."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910940b-9a25-4134-a4e9-765234a0e2f8",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "“**Data**” are, ultimately, information. If we measure something, and it's always the same, then there is no information. Difference is the heart of information. But, differences can be meaningful in some way (signal), or meaningless, such as variation due to random factors (noise). Information are those differences that mean something — that make a difference. Meaning can be derived in many ways, such as through visualizations (graphing), statistical analysis, or making predictions about future data. These are all ways of identifying systematic patterns in data.\n",
    "\n",
    "For the purposes of this class we assume that the data are collected in some way (measured), and stored digitally in files, typically as numbers or characters (words, sentences, etc.) — remembering that even digital media like movies or sounds are, ultimately, files composed of numbers. Data should be, however, more than just a bunch of numbers. Ultimately, data science is about identifying meaningful patterns in data. This informs our definition of \"data\" — data are sets of measurements *from which we aim to extract meaning*. As such, we assume that the data have been collected in such a way that meaning can be derived from them.\n",
    "\n",
    "In doing data science, we focus less on the process of acquiring the data, and more on what we do after it's collected. That said, it is *critically important* to understand what your data are — which includes *what* was being measured, and *how* it was measured. Often we might also care about *why* the data were measured, but not necessarily; increasingly, researchers are making data sets openly available (e.g., through public repositories such as [OSF.org](https://osf.org) not only for purposes of transparency, but with the expectation that other researchers might be able to use the data in ways other than originally intended, to generate new insights.  \n",
    "\n",
    "Our approaches to working with data will necessarily be different, according to our understanding of what was measured, the underlying physiological properties, and our goals — the meaning we are trying to derive from the data. At the same time, all of these are ultimately measurements stored in files on a computer, and data science is about learning the core skills that allow you to work with any of these types of data and try to find meaning in them.\n",
    "\n",
    "One final thought on the definition of data: since data is a combination of signal and noise, data may need various kinds of preparation or \"cleaning\" to strip away noise and more easily identify systematic patterns. When some people first encounter such practices, they raise questions about the validity of such practices, or whether they are \"cooking\" the data — manipulating it to generate the results the researcher desires. There is a huge, and fundamental, difference between manipulating data to generate a specific, predicted result, and cleaning data to minimize noise and optimize finding the signal. Approaches to data cleaning should be systematic, well-reasoned, and accurately reported. In data science, data cleaning procedures are typically well-understood and supported by the peer-reviewed scientific literature. In contrast, manipulating the data to achieve specific ends typically involves dishonest practices such as arbitrarily removing or adding data to create a data set that generates specific results (often without reporting how or why the data were thus manipulated). Dishonest practices are not accepted in the scientific community, and people caught manipulating their data are typically publicly discredited (or at least privately discredited, within social networks), and subsequently find it hard to obtain work or the respect of their peers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd186c8-3321-427a-9a83-cc986bdaf3b3",
   "metadata": {},
   "source": [
    "# Tools for Data Science\n",
    "\n",
    "Many people's experience working with data prior to this course is likely to have been in spreadsheet applications, like Microsoft Excel or Google Sheets. This section discusses the limitations of spreadsheets for conducting reproducible data science, and introduces the core concepts of scientific programming languages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e0a298-2bd0-4ae6-9652-7de14d1f9bfb",
   "metadata": {},
   "source": [
    "# Spreadsheets\n",
    "\n",
    "A wide variety of software packages and platforms are used in data science. You have probably worked with some of these in the past. Spreadsheets provide a **graphical user interface (GUI)** that allows the user to enter data into a **table**, which is composed of individual **cells** organized into **rows** and **columns**. Cells can contain different types of data, like numbers, letters, words, and sentences. Cells can also contain more complex data, like equations that calculate values, often based on data contained in other cells. Spreadsheets can be used to create visualizations of data, such as graphs, and perform simple statistical analyses, such as *t*-tests.\n",
    "\n",
    "Spreadsheets are useful tools for quickly inspecting data, and manually entering data if you need to do that. However, they are limited in many ways. To illustrate some of these limitations, let's take a simple example. Imagine you ran a simple behavioural reaction time (RT) experiment, called a \"flanker\" task. In this experiment, participants had to press either the left or right arrow key, to indicate whether an arrow shown on the screen is pointing left or right, respectively. However, the catch is that the centre arrow is \"flanked\" by two other arrows on each side; these can be pointing the same way as the target arrow (congruent):\n",
    "\n",
    "![flanker_congruent](images/flanker_congruent@0.75x.png)\n",
    "\n",
    "or in the opposite direction (incongruent)\n",
    "\n",
    "![flanker_incongruent](images/flanker_incongruent@0.75x.png)\n",
    "\n",
    "We expect that the incongruent condition would be associated with slower RTs, because the flanking arrows create some visual confusion and response competition (cognitive interference; this process is not unlike that underlying the famous [Stroop](https://en.wikipedia.org/wiki/Stroop_effect) experiment, which you are likely familiar with). In order to confirm this hypothesis, we would want to get an estimate of the RT for each condition, from a representative sample of human participants. We need a number of participants because we know that there is variability in the average RT from person to person (**between-subject** variability). For our example, let's say we tested 40 participants.\n",
    "\n",
    "To estimate the average RT for each condition for an individual, we would want to present many trials of each condition, in random order. We do this because there is always some trial-to-trial variability in measuring human RTs (**within-subject** variability). We might want to include 50 trials per condition.\n",
    "\n",
    "We normally run experiments like this on a computer, which would save a data file for each participant. So at the end of data collection, you now have 40 data files; one from each participant. An example data file is shown in the figure below, showing the first 10 trials for one participant (identified as `subj_01`) as it might appear in Microsoft Excel.\n",
    "\n",
    "![example spreadsheet](images/spreadsheet_RT_data.png)\n",
    "\n",
    "In this file, each row represents a trial, except for the first row, which is the **header** that labels each column. Each data row (numbered 2-11 in the spreadsheet) contains columns identifying the participant; the trial number, the condition (congruent or incongruent), and the RT (in seconds).\n",
    "\n",
    "Since our goal is to estimate the average RT for each condition, we could use Excel's `AVERAGE` function for this. However, we can't simply put the formula\n",
    "`=AVERAGE(D2:D101)` below the bottom row (remember, we ran 100 trials, even though only the first 10 are shown in the example above). This would give us the average RT for that participant, but would not provide separate RTs for each condition. Since the conditions are randomly intermixed, calculating the RT for each condition is actually a bit challenging. However, we could use Excel's \"sort\" function to sort the data by column C, which would result in all the congruent trials coming first, and the incongruent ones after. Then, we could insert two `AVERAGE` formulas, one for each condition.\n",
    "\n",
    "Since in our example we tested 40 participants, we would have to manually perform this process 40 times: open a data file, sort by column C, type in two formulas, and computer the average RTs for each condition. Then we would copy and paste these into a new spreadsheet, that had one row and two columns (mean congruent RT, mean incongruent RT) for each subject. Once we had done this 50 times, we could use Excel's `AVERAGE` function again to compute the mean RT across participants for each condition. To please our stats teacher, we might also compute the standard deviation (a measure of variability) across the subjects, which would be another equation put in another cell.\n",
    "\n",
    "```{Note}\n",
    "There is nothing fundamentally incorrect with this process, and it would yield the correct values for the data. However, there are a number of issues with this workflow. Before reading on, try to think of some!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a11a75-be30-45d2-9640-3475778f3d97",
   "metadata": {},
   "source": [
    "# Limitations of Spreadsheets\n",
    "\n",
    "...OK, so here are some issues with this \"manual\" process of doing data science:\n",
    "\n",
    "Firstly, it's tedious. You are doing the same thing — computing the average RT for each participant and condition — 100 times. While maybe you can get into a groove — with some chill music playing and a nice cup of coffee — it's still tedious and probably not the best use of your time. On top of that, it's **error prone**. You might make typos, you might accidentally miss including some rows for some subjects, you might forget to do one subject's data file, you might accidentally swap the congruent and incongruent values for one participant during the copying process, etc.. Wouldn't it be great if there was a machine that could do this automatically, and far more quickly and accurately than you? Then you would have more free time to just enjoy that coffee and music!\n",
    "\n",
    "Another issue is that the manual process is not readily **scalable**: if you ran another experiment with 100 participants, you would be bringing twice as much boredom, tedium, and risk of human error as in your first experiment.\n",
    "\n",
    "Finally, and perhaps most importantly, the manual process is not easily **reproducible**. That is to say, if you ran a new experiment, you would have to perform the same tedious process all over again, with the same risks of human error and terminal boredom. And if you wanted someone else to do the analysis for you, you would need to write out instructions and hope that they followed those instructions exactly. Furthermore, as often happens, if you were a student in the lab who graduates and moves on before the study is finished, someone else may be assigned to finish the data analysis, and they would have to figure out what you did, how you organized your files, etc.. In many cases when this happens, people have to start from scratch because the previous person's process was not clearly documented, and it's very hard to check for errors except by going through a large number of files.\n",
    "\n",
    "There has to be a better way!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4ee59-7c60-4091-8490-0ed160cda5ff",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "\n",
    "A fundamental principle of empirical (experimental) science is **reproducibility**. Scientific results should not be flukes, they should be based on documented and replicable processes. When we report the results of an experiment, we typically present a written description of the methods, as well as written and graphical reports of the results. In principle, these descriptions should be sufficient for a reader to reproduce your experiment, and hopefully get similar results. Of course, in kinesiological research, each experiment typically involves a new sample of participants, so even if the experiment is reproduced exactly, and the data analyzed identically, we can expect some variability in the results because we sampled a different set of individuals. However, if we take a copy of the original data, we should be able to produce the same results by following the documented procedures. This is one of the fundamental principles of **open science**.\n",
    "\n",
    "In practice, this is harder than it sounds — especially if we are using the manual spreadsheet approach described above. Unless the analysis was documented very closely, it's possible that methodological differences will arise. For example, a Methods section might state that the mean RT was calculated for each participant and then averaged across participants, but this doesn't specify that this was done by a lab volunteer cutting and pasting numbers while simultaneously watching YouTube and chatting with another student in the lab. Even if the procedures were precisely documented, replicating the process would still be as tedious and error-prone — and very likely, even the errors would be different since they are random occurrences.\n",
    "\n",
    "Science should not be this way; science should be accurate, precise, reliable, and reproducible. For this to be true, we need high standards of control and automation to ensure that data is handled consistently and reproducibly. If only we could replace those flaky lab volunteers with machines that did precisely what we intended...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3324b36-eccf-48d2-87b1-07f4201470fc",
   "metadata": {},
   "source": [
    "# Scientific Programming Languages\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Programming_language), a programming language is, “a formal language, which comprises a set of instructions that produce various kinds of output” — where “formal languages” are characterized by hierarchical organization in which letters are combined to form words, which are in turn combined into larger units according to rules called a **syntax** (or grammar). In general, programming languages are instructions for computers to perform.  There are thousands of programming languages in existence, which [Wikipedia attempts to catalogue on this page](https://en.wikipedia.org/wiki/List_of_programming_languages).\n",
    "\n",
    "There is nothing special about “scientific” programming languages to distinguish them from other programming languages, except that they are used for scientific purposes. Some languages, however, have become particularly widespread in scientific applications. Below is a discussion of different languages, but first to address the cliffhanger left at the end of the preceding section: programming languages provide a way to standardize and automate data analysis that is reproducible. Since programs are written sets of instructions stored in a file, the same set of instructions can be applied to every data file in a study, and if the programs used to analyze the data are shared with others, then others should be able to reproduce the original results. There is, of course, no guarantee that the original program was free of errors (“bugs”), but the fact that the instructions are written and saved means that they can be audited by others, which makes finding errors much easier (or possible at all) relative to a manual task performed by humans.\n",
    "\n",
    "Computer programs can also be written in ways to “batch” work, meaning that they can scale easily. For example, once a program has been written to process one data file in a desired way (e.g., compute an individual’s mean RTs for each condition, as in our example above), that program can be placed in a “loop” that applies the same process to every data file in a study. While running the program on each data file takes a certain amount of time — meaning more data will take longer to analyze — computer programs typically perform these kinds of routine tasks far faster than humans (often literally in the blink of an eye), and far more reliably. Where humans might make random errors, computer programs do not: if the program contains an error, it will systematically make the same error on every data file it processes. While errors are obviously not desirable, a systematic error is typically easier to detect and correct than random errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8e4ca-9c7a-49f1-8965-8f580660cd16",
   "metadata": {},
   "source": [
    "# Which language to use?\n",
    "\n",
    "Although there are thousands of programming languages, a relatively small number of them are widely used. Which languages are the most popular or commonly-used depends on the discipline and area of use. Different languages are designed for very different purposes, and in many cases new work builds on older work, so the use of a particular language in a particular setting will tend to cause that language to propagate within that setting. For example, some languages are well-suited to building interactive web sites, while others may be more suitable for building apps for mobile devices, and others for writing code to be embedded in hardware devices.  One of the most long-standing and representative indices of programming language popularity is the [TIOBE web site](https://www.tiobe.com/tiobe-index/), whose ratings are based on “the number of skilled engineers world-wide, courses and third party vendors. Popular search engines such as Google, Bing, Yahoo!, Wikipedia, Amazon, YouTube and Baidu are used to calculate the ratings.” As of April, 2020, the 10 most popular languages are (in order): Java, C, Python, C\\++, C#, Visual Basic, JavaScript, PHP, SQL, and R. (Don’t worry if you haven’t heard of all of these — there won’t be a test! This is merely to give you a sense of the “lay of the land”, and expose you to the names of languages you’re likely to come across in the future.)\n",
    "\n",
    "In data science, a few languages are particularly widely used. The internet is rife with clickbait-y pages such as “Top languages every data scientist should know”; while these may rely on questionable methodologies, a general survey of such pages reveals a fairly consistent set of languages, including Python, R, MATLAB, C, Java, SQL, Julia, and Scala. \n",
    "\n",
    "It is important to know that there is no one, “best,” programming language — either for programming in general, or for kinesiology in particular. Indeed, many scientists have workflows that include multiple languages. For example, some of my own lab's research involves a robotic device that is controlled through Matlab/Simulink programs. For these studies, we collect the data through Matlab, but rely on Python to process the data and to perform the statistics. However, other neuromechanics labs may use different workflows, such as MATLAB for processing and SPSS for statistics. \n",
    "\n",
    "So the punch line is, you should use the language that is best-suited for the task at hand. In the example of my lab’s workflow, we use Python for data processing because I prefer Python as a language to work in for some of the reasons described below. And we perform the statistics using Python as well, because I would like to keep the language consistent across as many aspects of our data analysis pipeline as possible, and I have also written [an open, publicly available Python library](https://github.com/hyosubkim/bayesian-statistics-toolbox) (a library is simply a set of tools written to extend functionality and perform particular tasks) for running the sorts of statistical models we prefer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc656ee-0d6e-4345-9f41-8f34a09f498f",
   "metadata": {},
   "source": [
    "# Why Python for this Course?\n",
    "\n",
    "With all that said, I have chosen to use Python in this course, for several reasons. Firstly, it is quickly becoming one of the most widely used languages in modern science, generally, and it is being picked up in greater and greater numbers each year within kinesiology as well. In addition, it is one of the  most, if not *the* most, widely used programming language in non-academic data science applications (i.e., industry), so it will serve you well in the future whether you extend your academic career or decide to go into industry, health care, government, etc. Secondly, it is an extremely well-designed language, with syntax and structure that many find much easier to understand than other high-level languages, like R or Matlab. Thirdly, it is an **open-source** language, meaning that it is free to obtain and use. This is also true of R, and indeed most widely-used programming languages, but MATLAB is closed-source: it is developed and sold by The Mathworks, a private company. While UBC pays for a site license that allows all students and staff to use it, this is still a potential limitation for you in the future, and it is also inconsistent with a core principle of this class, which is to use and promote open-source tools and resources as much as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb5e47-85d3-4715-942e-a68598e89e69",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"images/python-logo-master-v3-TM.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "This class will teach you to understand and use the [Python](https://www.python.org) programming language, along with a set of libraries commonly used in data science broadly. I do not expect that you’ve ever written a line of code in any programming language before, so in learning to use Python, you will also be learning to program. Using Python for data science, and programming, are not exactly the same thing — programming describes a broader range of skills than using a programming language to do data science. As well as learning the \"words\" (commands) and \"grammar\" (how to define and combine commands), of a programming language, programming encompasses particular ways of thinking. One important programming skill is **operationalization** — analyzing and breaking down problems, and identifying the sequence of steps to solve them. Another is paying close attention to the details of how you write and format your code (all of the sudden, not indenting a line is not just a violation of that annoying APA Style guide, but causes your code to function in a totally different way, or not at all!).\n",
    "\n",
    "Python was originally written by [Guido van Rossum](https://en.wikipedia.org/wiki/Guido_van_Rossum) and first released in 1991.  Its name has nothing to do with snakes, but rather was derived from the famous comedy sketch troupe Monty Python. Python developed as an [**open-source**](https://en.wikipedia.org/wiki/Open-source_software) project. This means several things. Firstly, that it is made available for free, with anyone being granted the permission to use, examine, modify, and share the [**source code**](https://en.wikipedia.org/wiki/Source_code) (the code that runs when you run a Python command). Secondly, that many people contributed to the development of the language, typically without receiving any payment (though some developers may have contributed to Python in the context of working for a company that relied on the language, or simply embraced values of supporting the open-source community). Van Rossum was the lead developer for the project until 2018, and now the development of the language is guided by a five-person steering council (which still includes Van Rossum). Like virtually every active programming language, Python is under continual development, to fix bugs, improve its efficiency, and extend its abilities. Python has gone through three major versions, each with many minor releases. Development is guided by officially reviewed and approved [Python Enhancement Proposals](https://www.python.org/dev/peps/) (PEP). Some PEPs also serve as official guidelines. For example, PEP 20 is [The Zen of Python](https://www.python.org/dev/peps/pep-0020/), which espouses core values of the language, while PEP 8 is [The Style Guide for Python Code](https://www.python.org/dev/peps/pep-0008/), which we will return to later and often as it defines rules concerning how the code is interpreted (e.g., indents, as mentioned above), as well as guidelines that make code consistent and easy to read.\n",
    "\n",
    "In a very general and nontechnical sense, programming languages can be characterized as falling on a continuum from “higher level” to “lower level” (or, perhaps more simply, easier to use and learn to harder to use and learn). Python falls closer to the “high level” end of this spectrum, relative to languages like C or Java. This often means it takes less code to perform a particular function, more things are baked in \"for free\" in Python than one might have to explicitly write code for in C. As a result, Python is simpler and more elegant to read and write. Indeed, PEP 20 enshrines certain core values of the language, such as:\n",
    "\n",
    "*Beautiful is better than ugly.*\n",
    "\n",
    "*Explicit is better than implicit.*\n",
    "\n",
    "*Simple is better than complex.*\n",
    "\n",
    "*Readability counts.*\n",
    "\n",
    "These values contribute to making Python relatively easy to learn and use, compared to other programming languages. At the same time, programs written in Python (if written properly) tend to run quickly and efficiently, so there is little \"overhead\" relative to using a lower-level language. Python has been widely adopted by communities in many areas of science, and in data science, because of this (and the fact that it's free). Many add-on packages (**libraries**) have been written to extend Python's functionality in various ways, including a large number of libraries specifically for scientific applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2b02a-09fd-4d6f-8114-fd812539e3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
